<script type="text/javascript" async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

## 算法收敛性

定理(Novikoff):设训练数据集$$T=\{(x_{1},y_{1}),(x_{2},y_{2}),....,(x_{n},y_{n})\}$$是线性可分的，其中$$x_{i} \in X=R^{n},y_{i} \in Y=\{-1,1\},i=1,2,...N$$,则

（1）存在满足条件的$$\parallel \hat{w}_{opt} \parallel =1$$的超平面$$\hat{w}_{opt} \cdot \hat{x} =w_{opt} \cdot x + b_{opt}=0$$将训练数据集完全正确分开；且存在$$\gamma >0$$,对所有i=1,2,...N。
$$y_{i}(\hat{w}_{opt}\cdot\hat{x_{i}})=y_{i}(w_{opt}\cdot x_{i}+b_{opt}) \ge \gamma$$

（2）令$$R=max_{1 \le i \le N}\parallel\hat{x}_{i}\parallel$$,则感知机算法2.1在训练数据集上的误分类次数k满足不等式

$$k \le (\frac{R}{\gamma})^{2}$$

证明：

（1）由于训练数据集是线性可分的，按照定义2.2，存在超平面可将训练数据集完全正确分开，取超平面为$$\hat{w_{opt}}\cdot\hat{x}=w_{opt}\cdot x+b_{opt}=0,使 \parallel\hat{w}_{opt}\parallel=1$$.由于对有限的$$i=1,2,....,N$$均有$$y_{i}(\hat{w}_{opt}\cdot\hat{x}_{i})=y_{i}(w_{opt}\cdot x_{i}+b_{opt})>0$$,所以存在$$\gamma=min_{i}\{y_{i}(w_{opt}\cdot x_{i}+b_{opt})\}$$使得$$y_{i}(\hat{w}_{opt}\cdot\hat{x}_{i})=y_{i}(w_{opt}\cdot x_{i}+b_{opt}) \ge \gamma$$

（2）感知机从$$\hat{w}_{0}=0$$开始，如果实例被误分类，则更新权重。令$$\hat{w}_{k-1}$$是第k个误分类实例之前的扩充权重向量$$\hat{w}_{k-1}=(w^{T}_{k-1},b_{k-1})^{T}$$那么第k个误分类实例的条件是

$$y_{i}(\hat{w}_{k-1}\cdot \hat{x}_{i})=y_{i}(w_{k-1}\cdot x_{i}+b_{k-1})\le 0$$

如何$$(x_{i},y_{i})是被\hat{w}_{k-1}=(w^{T}_{k-1}.b_{k-1})^{T}$$误分类的数据，那么w和b的更新是

$$w_{k}\leftarrow w_{k-1}+\eta y_{i}x_{i},b_{k}\leftarrow b_{k-1}+\eta y_{i}$$

由上面公式得到$$\hat{w}_{k}\cdot\hat{w}_{opt}=\hat{w}_{k-1}\cdot\hat{w}_{opt}+\eta y_{i}\hat{w}_{opt}\cdot\hat{x}_{i}\le\hat{w}_{k-1}\cdot\hat{w}_{opt}+\eta\gamma\le\hat{w}_{k-2}\cdot\hat{w}_{opt}+2\eta\gamma\le ... \le k\eta\gamma$$

同样由上面公式可得：$$\parallel\hat{w}_{k}\parallel^{2}=\parallel\hat{w}_{k-1}\parallel^{2}+2\eta y_{i}\hat{w}_{k-1}\cdot\hat{x}_{i}+\eta^{2}\parallel\hat{x}_{i}\parallel^{2}\le\parallel\hat{w}_{k-1}\parallel^{2}+\eta^{2}\parallel\hat{x}_{i}\parallel^{2}$$

$$\le\parallel\hat{w}_{k-2}\parallel^{2}+2\eta R^{2} \le ...\le k\eta^{2}R^{2}$$

($$\parallel\hat{x}_{i}^{2}\parallel=R^{2}这里是因为\eta >0,y_{i}\hat{w}_{k-1}\cdot\hat{x}_{i}\le 0$$)

从上面两个不等式可以得到

$$k\eta\gamma\le\hat{w}_{k}\cdot\hat{w}_{opt}\le\parallel\hat{w}_{k}\parallel\parallel\hat{w}_{opt}\parallel\le\sqrt{k}\eta R\rightarrow k^{2}\gamma^{2}\le kR^{2}$$

$$\rightarrow k\le (\frac{R}{\gamma})^{2}$$

当训练数据集线性可分时，感知机学习算法原始形式迭代是收敛的。